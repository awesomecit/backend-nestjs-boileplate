# üß™ BATS Infrastructure Testing - Guida Completa

## üìÅ Posizione File

**Path**: `docs/development/infrastructure-testing.md`

---

# üî¨ BATS Infrastructure Testing: Da Zero a Expert

> **BATS (Bash Automated Testing System)** √® il cuore del nostro approccio TDD Infrastructure.
> Questa guida ti trasformer√† da principiante a esperto nella scrittura di test infrastructure robusti e affidabili.

## üéØ Perch√© BATS? L'Evoluzione del Testing Infrastructure

### ü§î Il Problema Tradizionale

```mermaid
graph TD
    A[Deploy Script] --> B{Funziona?}
    B -->|No| C[Debug Manuale]
    B -->|S√¨| D[Deploy Produzione]
    D --> E{Problemi Produzione?}
    E -->|S√¨| F[Hotfix Urgente]
    E -->|No| G[Tutto OK... per ora]

    C --> H[Fix Script]
    H --> A
    F --> A

    style C fill:#ffebee
    style F fill:#ffebee
    style E fill:#fff3e0
```

**Problemi dell'approccio tradizionale:**

- Testing manuale lento e soggetto a errori
- Script "speranza-driven" senza validazione
- Debug in produzione costoso e rischioso
- Nessuna regressione prevention
- Knowledge silos (solo chi ha scritto lo script sa come funziona)

### ‚úÖ La Soluzione BATS + TDD

```mermaid
graph TD
    A[Requirement] --> B[Scrivi Test BATS]
    B --> C[Test Fallisce RED]
    C --> D[Implementa Script]
    D --> E[Test Passa GREEN]
    E --> F[Refactor BLUE]
    F --> G[Deploy Confidence]

    G --> H{Nuovo Requirement?}
    H -->|S√¨| A
    H -->|No| I[Produzione Stabile]

    style B fill:#e8f5e8
    style E fill:#e8f5e8
    style G fill:#e3f2fd
    style I fill:#e8f5e8
```

**Vantaggi dell'approccio TDD:**

- **Confidence**: Ogni deploy √® validato automaticamente
- **Documentation**: I test sono documentazione eseguibile
- **Regression Prevention**: Modifiche future non rompono funzionalit√† esistenti
- **Knowledge Sharing**: Chiunque pu√≤ capire cosa fa l'infrastruttura leggendo i test
- **Faster Debugging**: Problemi identificati immediatamente, non in produzione

## üèóÔ∏è Architettura BATS nel Nostro Progetto

### üìÇ Struttura Directory Test

```
test/
‚îú‚îÄ‚îÄ üìÅ helpers/                    # Funzioni riutilizabili
‚îÇ   ‚îú‚îÄ‚îÄ docker-helpers.bash        # Gestione Docker/Swarm
‚îÇ   ‚îú‚îÄ‚îÄ monitoring-helpers.bash    # Setup Prometheus/Grafana
‚îÇ   ‚îú‚îÄ‚îÄ logging-helpers.bash       # Validazione ELK Stack
‚îÇ   ‚îî‚îÄ‚îÄ common-helpers.bash        # Utilities generiche
‚îú‚îÄ‚îÄ üìÅ infrastructure/             # Test Infrastructure
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ cluster/               # Docker Swarm Tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test-cluster-init.bats
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test-cluster-scaling.bats
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test-cluster-recovery.bats
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ monitoring/            # Monitoring Stack Tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test-prometheus.bats
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test-grafana.bats
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test-alerts.bats
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ logging/               # Logging Infrastructure Tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test-elk-stack.bats
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test-log-aggregation.bats
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test-curl-replication.bats
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ deployment/            # CI/CD Pipeline Tests
‚îÇ       ‚îú‚îÄ‚îÄ test-deployment.bats
‚îÇ       ‚îú‚îÄ‚îÄ test-rollback.bats
‚îÇ       ‚îî‚îÄ‚îÄ test-blue-green.bats
‚îú‚îÄ‚îÄ üìÅ fixtures/                   # Dati e configurazioni test
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.test.yml
‚îÇ   ‚îú‚îÄ‚îÄ prometheus.test.yml
‚îÇ   ‚îî‚îÄ‚îÄ sample-logs.json
‚îî‚îÄ‚îÄ üìÅ reports/                   # Report test eseguiti
    ‚îú‚îÄ‚îÄ infrastructure-report.xml
    ‚îî‚îÄ‚îÄ coverage-report.html
```

### üîÑ Flusso di Esecuzione Test

```mermaid
sequenceDiagram
    participant Dev as Developer
    participant BATS as BATS Runner
    participant Docker as Docker Engine
    participant Infra as Infrastructure

    Dev->>BATS: bats test/infrastructure/
    BATS->>BATS: Load helpers/
    BATS->>BATS: Setup test environment

    loop For each test
        BATS->>Docker: Execute infrastructure commands
        Docker->>Infra: Create/Configure resources
        Infra-->>Docker: Status/Response
        Docker-->>BATS: Command result
        BATS->>BATS: Assert expectations
    end

    BATS->>BATS: Cleanup test environment
    BATS-->>Dev: Test results & report
```

## üìö BATS Syntax: Dal B√°sico all'Avanzato

### üå± Livello Base: Primi Test

```bash
#!/usr/bin/env bats

# Test pi√π semplice possibile
@test "Docker √® disponibile" {
    run docker --version
    [ "$status" -eq 0 ]
}

# Test con messaggio di errore personalizzato
@test "Node.js versione corretta" {
    run node --version
    [ "$status" -eq 0 ]
    [[ "$output" =~ "v18" ]] || fail "Expected Node.js v18, got: $output"
}
```

### üåø Livello Intermedio: Setup e Helpers

```bash
#!/usr/bin/env bats

# Caricamento helper functions
load '../helpers/docker-helpers'
load '../helpers/common-helpers'

# Setup eseguito prima di ogni test
setup() {
    # Cleanup ambiente precedente
    cleanup_docker_environment

    # Setup variabili test
    export TEST_CONTAINER_NAME="test-app-$(random_string 8)"
    export TEST_NETWORK="test-network-$(random_string 8)"
}

# Teardown eseguito dopo ogni test
teardown() {
    # Cleanup delle risorse create
    docker container rm -f "$TEST_CONTAINER_NAME" 2>/dev/null || true
    docker network rm "$TEST_NETWORK" 2>/dev/null || true
}

@test "Container si avvia con configurazione corretta" {
    # Given: Configurazione container
    local image="nginx:alpine"
    local port="8080"

    # When: Avvio container
    run docker run -d \
        --name "$TEST_CONTAINER_NAME" \
        --network "$TEST_NETWORK" \
        -p "$port:80" \
        "$image"

    # Then: Verifiche multiple
    [ "$status" -eq 0 ]

    # Container √® running
    run docker ps --filter "name=$TEST_CONTAINER_NAME" --format "{{.Status}}"
    [[ "$output" =~ "Up" ]]

    # Porta √® accessibile
    run curl -s -o /dev/null -w "%{http_code}" "http://localhost:$port"
    [ "$output" = "200" ]
}
```

### üå≥ Livello Avanzato: Pattern Complessi

```bash
#!/usr/bin/env bats

load '../helpers/docker-helpers'
load '../helpers/monitoring-helpers'

# Setup pi√π complesso con retry logic
setup() {
    export SWARM_CLUSTER_NAME="test-cluster-$(random_string 8)"
    export PROMETHEUS_PORT="9090"
    export GRAFANA_PORT="3000"

    # Inizializza cluster con retry
    retry 3 init_swarm_cluster "$SWARM_CLUSTER_NAME"
}

@test "Monitoring stack si deploy completamente e comunica" {
    # Given: Stack monitoring definito
    local stack_file="test/fixtures/monitoring-stack.yml"
    local stack_name="monitoring-test"

    # When: Deploy stack
    run docker stack deploy -c "$stack_file" "$stack_name"
    [ "$status" -eq 0 ]

    # Then: Verifica servizi sono healthy
    local services=("prometheus" "grafana" "node-exporter")

    for service in "${services[@]}"; do
        # Attendi servizio ready con timeout
        wait_for_service_ready "$stack_name"_"$service" 300

        # Verifica replicas attese
        local expected_replicas=$(get_service_replicas "$stack_name"_"$service")
        local actual_replicas=$(get_running_replicas "$stack_name"_"$service")
        [ "$actual_replicas" -eq "$expected_replicas" ]
    done

    # Verifica comunicazione tra servizi
    run test_prometheus_targets_healthy
    [ "$status" -eq 0 ]

    # Verifica Grafana pu√≤ interrogare Prometheus
    run test_grafana_datasource_connection
    [ "$status" -eq 0 ]
}

# Test di scenario complesso: Auto-scaling
@test "Auto-scaling risponde a carico CPU elevato" {
    # Given: Servizio con auto-scaling configurato
    local service_name="web-app"
    local initial_replicas=3
    local max_replicas=10

    deploy_service_with_autoscaling "$service_name" "$initial_replicas" "$max_replicas"

    # When: Simula carico CPU elevato
    run generate_cpu_load "$service_name" 80  # 80% CPU
    [ "$status" -eq 0 ]

    # Then: Verifica scaling automatico
    # Attendi scaling trigger (max 300 secondi)
    local timeout=300
    local start_time=$(date +%s)

    while true; do
        local current_replicas=$(get_running_replicas "$service_name")

        # Scaling avvenuto?
        if [ "$current_replicas" -gt "$initial_replicas" ]; then
            break
        fi

        # Timeout check
        local current_time=$(date +%s)
        local elapsed=$((current_time - start_time))
        if [ "$elapsed" -gt "$timeout" ]; then
            fail "Auto-scaling non avvenuto entro $timeout secondi"
        fi

        sleep 10
    done

    # Verifica che scaling sia proporzionale al carico
    local final_replicas=$(get_running_replicas "$service_name")
    [ "$final_replicas" -le "$max_replicas" ]
    [ "$final_replicas" -gt "$initial_replicas" ]
}
```

## üõ†Ô∏è Helper Functions: La Libreria del Successo

### üéØ Principio DRY (Don't Repeat Yourself)

```bash
# ‚ùå ANTI-PATTERN: Duplicazione codice
@test "Test 1" {
    docker swarm init --advertise-addr $(docker info --format '{{.Swarm.NodeAddr}}')
    # ... resto del test
}

@test "Test 2" {
    docker swarm init --advertise-addr $(docker info --format '{{.Swarm.NodeAddr}}')
    # ... resto del test
}

# ‚úÖ PATTERN CORRETTO: Helper function
# In helpers/docker-helpers.bash:
init_swarm_cluster() {
    local advertise_addr
    advertise_addr=$(get_primary_ip)
    docker swarm init --advertise-addr "$advertise_addr"
}

# Nei test:
@test "Test 1" {
    init_swarm_cluster
    # ... resto del test
}
```

### üß© Esempio Helper Function Completa

```bash
# File: test/helpers/docker-helpers.bash

# Inizializza cluster Swarm con error handling
init_swarm_cluster() {
    local advertise_addr="${1:-}"

    # Se non specificato, detecta IP automaticamente
    if [[ -z "$advertise_addr" ]]; then
        advertise_addr=$(get_primary_ip)
    fi

    # Verifica che Docker sia disponibile
    if ! validate_docker_available; then
        return 1
    fi

    # Se Swarm gi√† attivo, fai leave prima
    if is_swarm_active; then
        docker swarm leave --force >/dev/null 2>&1 || true
    fi

    # Inizializza nuovo cluster
    docker swarm init --advertise-addr "$advertise_addr" >/dev/null 2>&1

    # Verifica successo
    is_swarm_active
}

# Verifica se Swarm √® attivo
is_swarm_active() {
    docker info --format '{{.Swarm.LocalNodeState}}' 2>/dev/null | grep -q "active"
}

# Ottieni IP primario della macchina
get_primary_ip() {
    # Prova diversi metodi per robustezza
    local ip

    # Metodo 1: Via default route
    ip=$(ip route get 1.1.1.1 2>/dev/null | awk '{print $7; exit}')
    if [[ -n "$ip" && "$ip" =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
        echo "$ip"
        return 0
    fi

    # Metodo 2: Via hostname
    ip=$(hostname -I 2>/dev/null | awk '{print $1}')
    if [[ -n "$ip" && "$ip" =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
        echo "$ip"
        return 0
    fi

    # Fallback: localhost (per testing)
    echo "127.0.0.1"
}

# Cleanup completo ambiente Docker
cleanup_docker_environment() {
    # Stop e remove tutti i container
    local containers
    containers=$(docker ps -aq 2>/dev/null)
    if [[ -n "$containers" ]]; then
        docker stop $containers >/dev/null 2>&1 || true
        docker rm $containers >/dev/null 2>&1 || true
    fi

    # Leave swarm se attivo
    if is_swarm_active; then
        docker swarm leave --force >/dev/null 2>&1 || true
    fi

    # Cleanup networks custom
    local networks
    networks=$(docker network ls --filter "driver=overlay" --format "{{.Name}}" 2>/dev/null | grep -v "ingress")
    if [[ -n "$networks" ]]; then
        echo "$networks" | xargs -r docker network rm >/dev/null 2>&1 || true
    fi

    # Cleanup volumes orphan
    docker volume prune -f >/dev/null 2>&1 || true
}

# Attendi che servizio sia ready
wait_for_service_ready() {
    local service_name="$1"
    local timeout="${2:-120}"  # Default 2 minuti
    local interval="${3:-5}"   # Check ogni 5 secondi

    local start_time=$(date +%s)

    while true; do
        # Verifica stato servizio
        local ready_replicas
        ready_replicas=$(docker service ls --filter "name=$service_name" --format "{{.Replicas}}" 2>/dev/null)

        if [[ "$ready_replicas" =~ ^([0-9]+)/\1$ ]]; then
            # Tutte le repliche sono ready
            return 0
        fi

        # Check timeout
        local current_time=$(date +%s)
        local elapsed=$((current_time - start_time))
        if [[ "$elapsed" -gt "$timeout" ]]; then
            return 1
        fi

        sleep "$interval"
    done
}

# Genera stringa casuale per naming
random_string() {
    local length="${1:-8}"
    head /dev/urandom | tr -dc a-z0-9 | head -c "$length"
}

# Retry command con backoff
retry() {
    local attempts="$1"
    shift
    local cmd=("$@")

    for ((i=1; i<=attempts; i++)); do
        if "${cmd[@]}"; then
            return 0
        fi

        if [[ "$i" -lt "$attempts" ]]; then
            local delay=$((i * 2))  # Exponential backoff
            sleep "$delay"
        fi
    done

    return 1
}
```

## üéØ Pattern TDD Infrastructure

### üî¥ RED Phase: Writing Failing Tests

```bash
# Test scenario: "Il sistema deve supportare zero-downtime deployment"
@test "Zero-downtime deployment mantiene servizio disponibile" {
    # Given: Servizio in running
    local service_name="web-api"
    local version_1="app:v1.0"
    local version_2="app:v2.0"

    deploy_service "$service_name" "$version_1" 3
    wait_for_service_ready "$service_name"

    # When: Upgrade a nuova versione
    run perform_rolling_update "$service_name" "$version_2"
    [ "$status" -eq 0 ]

    # Then: Servizio sempre raggiungibile durante update
    run verify_continuous_availability "$service_name" 300  # 5 minuti
    [ "$status" -eq 0 ]

    # And: Alla fine tutte le istanze sono versione nuova
    run verify_all_instances_version "$service_name" "$version_2"
    [ "$status" -eq 0 ]
}
```

**‚ùå Primo run**: Test fallisce perch√© `perform_rolling_update` e `verify_continuous_availability` non esistono ancora.

### üü¢ GREEN Phase: Minimal Implementation

```bash
# Implementazione minima in helpers/deployment-helpers.bash
perform_rolling_update() {
    local service_name="$1"
    local new_image="$2"

    # Update strategy pi√π semplice possibile
    docker service update --image "$new_image" "$service_name"
}

verify_continuous_availability() {
    local service_name="$1"
    local duration="$2"
    local endpoint="http://localhost:8080/health"

    local start_time=$(date +%s)
    local end_time=$((start_time + duration))

    while [[ $(date +%s) -lt $end_time ]]; do
        if ! curl -s -f "$endpoint" >/dev/null; then
            return 1  # Servizio non disponibile
        fi
        sleep 1
    done

    return 0
}
```

**‚úÖ Secondo run**: Test passa! Implementazione minima funziona.

### üîµ REFACTOR Phase: Optimization

```bash
# Versione ottimizzata con configuration avanzata
perform_rolling_update() {
    local service_name="$1"
    local new_image="$2"
    local update_parallelism="${3:-1}"
    local update_delay="${4:-10s}"

    # Rolling update con controllo granulare
    docker service update \
        --image "$new_image" \
        --update-parallelism "$update_parallelism" \
        --update-delay "$update_delay" \
        --update-failure-action rollback \
        --update-monitor 30s \
        "$service_name"

    # Attendi completamento update
    wait_for_update_completion "$service_name"
}

verify_continuous_availability() {
    local service_name="$1"
    local duration="$2"
    local check_interval="${3:-1}"
    local endpoint="http://localhost:8080/health"
    local max_consecutive_failures="${4:-3}"

    local start_time=$(date +%s)
    local end_time=$((start_time + duration))
    local consecutive_failures=0

    while [[ $(date +%s) -lt $end_time ]]; do
        if curl -s -f "$endpoint" >/dev/null; then
            consecutive_failures=0
        else
            ((consecutive_failures++))
            if [[ $consecutive_failures -ge $max_consecutive_failures ]]; then
                return 1
            fi
        fi
        sleep "$check_interval"
    done

    return 0
}
```

**‚úÖ Terzo run**: Test continua a passare, ma ora abbiamo implementazione production-ready!

## üîß Tool Integration e CI/CD

### üöÄ GitHub Actions Integration

```yaml
# File: .github/workflows/infrastructure-tests.yml
name: Infrastructure Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  infrastructure-tests:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Setup BATS
        run: |
          sudo apt-get update
          sudo apt-get install -y bats

      - name: Setup Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Run Infrastructure Tests
        run: |
          # Test sequenza organizzata
          echo "üîß Testing cluster initialization..."
          bats test/infrastructure/cluster/test-cluster-init.bats

          echo "üìä Testing monitoring stack..."
          bats test/infrastructure/monitoring/test-*.bats

          echo "üìã Testing logging infrastructure..."
          bats test/infrastructure/logging/test-*.bats

          echo "üöÄ Testing deployment pipeline..."
          bats test/infrastructure/deployment/test-*.bats

      - name: Generate Test Report
        if: always()
        run: |
          # Converte output BATS in JUnit XML per GitHub
          ./scripts/bats-to-junit.sh test/reports/

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: infrastructure-test-results
          path: test/reports/
```

### üìä Makefile Integration

```makefile
# Makefile: Comandi rapidi per sviluppatori

.PHONY: test-infra test-infra-quick test-infra-full

# Test rapido (solo cluster)
test-infra-quick:
 @echo "üîß Running quick infrastructure tests..."
 @bats test/infrastructure/cluster/test-cluster-init.bats

# Test completo infrastructure
test-infra:
 @echo "üß™ Running full infrastructure test suite..."
 @for dir in test/infrastructure/*/; do \
  echo "üìÅ Testing $$dir"; \
  bats "$$dir"test-*.bats; \
 done

# Test con cleanup forzato
test-infra-clean:
 @echo "üßπ Cleaning environment before tests..."
 @./scripts/cleanup-test-environment.sh
 @make test-infra

# Test con report dettagliato
test-infra-report:
 @echo "üìä Running tests with detailed reporting..."
 @mkdir -p test/reports
 @bats --formatter junit test/infrastructure/**/*.bats > test/reports/infrastructure-junit.xml
 @bats --formatter tap test/infrastructure/**/*.bats > test/reports/infrastructure-tap.log
 @echo "Reports generated in test/reports/"

# Validazione syntax tutti i file
validate-syntax:
 @echo "üîç Validating bash syntax..."
 @find test/ -name "*.bash" -o -name "*.bats" | xargs -I {} bash -n {}
 @find scripts/ -name "*.sh" | xargs -I {} bash -n {}
 @echo "‚úÖ All bash files have valid syntax"

# Help per sviluppatori
help:
 @echo "üìö Available infrastructure testing commands:"
 @echo "  test-infra-quick  - Run quick cluster tests only"
 @echo "  test-infra        - Run full infrastructure test suite"
 @echo "  test-infra-clean  - Clean environment and run tests"
 @echo "  test-infra-report - Run tests with detailed reporting"
 @echo "  validate-syntax   - Validate all bash/bats files syntax"
```

## üêõ Debugging e Troubleshooting

### üîç Tecniche di Debug BATS

#### 1. Debug Verboso

```bash
# Esegui con output dettagliato
bats -t test/infrastructure/cluster/test-cluster-init.bats

# Output esempio:
# test-cluster-init.bats
#  ‚úì Docker is available and running
#  ‚úó Docker Swarm cluster can be initialized
#    (in test file test-cluster-init.bats, line 23)
#      `[ "$status" -eq 0 ]' failed
#    Docker command failed with status: 1
#    Output: Error response from daemon: This node is already part of a swarm
```

#### 2. Debug con Print Statements

```bash
@test "Debug example" {
    # Debug variabili
    echo "DEBUG: TEST_VAR=$TEST_VAR" >&3

    # Debug comando prima dell'esecuzione
    echo "DEBUG: About to run: docker swarm init" >&3

    run docker swarm init --advertise-addr "$(get_primary_ip)"

    # Debug output comando
    echo "DEBUG: Command status: $status" >&3
    echo "DEBUG: Command output: $output" >&3

    [ "$status" -eq 0 ]
}
```

#### 3. Test Isolation per Debug

```bash
# Crea test minimo per isolare problema
@test "Minimal reproduction test" {
    # Solo il pezzo che non funziona
    run get_primary_ip
    [ "$status" -eq 0 ]
    [[ "$output" =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]]
}
```

### üö® Errori Comuni e Soluzioni

#### Errore: "Docker daemon not running"

```bash
# Problema
@test "This will fail" {
    run docker ps
    [ "$status" -eq 0 ]  # ‚ùå Fallisce se Docker non running
}

# Soluzione: Validazione preliminare
setup() {
    validate_docker_available || skip "Docker not available"
}

@test "This will work or skip" {
    run docker ps
    [ "$status" -eq 0 ]  # ‚úÖ Skip se Docker non disponibile
}
```

#### Errore: "Port already in use"

```bash
# Problema: Conflitti di porta tra test
@test "Service on port 8080" {
    run docker run -d -p 8080:80 nginx
    [ "$status" -eq 0 ]  # ‚ùå Fallisce se porta occupata
}

# Soluzione: Porte dinamiche
@test "Service on dynamic port" {
    local port
    port=$(get_free_port)

    run docker run -d -p "$port:80" nginx
    [ "$status" -eq 0 ]  # ‚úÖ Usa porta libera
}

# Helper function
get_free_port() {
    python3 -c "import socket; s=socket.socket(); s.bind(('', 0)); print(s.getsockname()[1]); s.close()"
}
```

#### Errore: "Resource cleanup incomplete"

```bash
# Problema: Residui test precedenti
teardown() {
    docker rm -f "$TEST_CONTAINER" 2>/dev/null || true  # ‚ùå Cleanup parziale
}

# Soluzione: Cleanup robusto
teardown() {
    # Cleanup completo con retry
    local containers_to_clean
    containers_to_clean=$(docker ps -aq --filter "label=test-session=$TEST_SESSION")

    if [[ -n "$containers_to_clean" ]]; then
        docker stop $containers_to_clean >/dev/null 2>&1 || true
        docker rm $containers_to_clean >/dev/null 2>&1 || true
    fi

    # Cleanup networks
    docker network prune -f >/dev/null 2>&1 || true

    # Cleanup volumes
    docker volume prune -f >/dev/null 2>&1 || true
}
```

## üìà Metriche e Monitoring Test

### üìä Test Performance Tracking

```bash
# File: test/helpers/performance-helpers.bash

# Misura tempo esecuzione comando
measure_execution_time() {
    local cmd=("$@")
    local start_time end_time duration

    start_time=$(date +%s.%N)
    "${cmd[@]}"
    local exit_code=$?
    end_time=$(date +%s.%N)

    duration=$(echo "$end_time - $start_time" | bc)
    echo "PERFORMANCE: Command '${cmd[*]}' took ${duration}s" >&3

    return $exit_code
}

# Test con SLA timing
@test "Service startup rispetta SLA (<30 secondi)" {
    local start_time
    start_time=$(date +%s)

    run deploy_service "web-app" "nginx:alpine" 3
    [ "$status" -eq 0 ]

    wait_for_service_ready "web-app" 30
    local wait_status=$?

    local end_time duration
    end_time=$(date +%s)
    duration=$((end_time - start_time))

    echo "SERVICE_STARTUP_TIME: ${duration}s" >&3
    [ "$wait_status" -eq 0 ]
    [ "$duration" -le 30 ]
}
```

### üìã Test Coverage Reporting

```bash
# Script: scripts/generate-test-coverage.sh
#!/bin/bash

echo "üìä Generating Infrastructure Test Coverage Report..."

# Conta test totali
total_tests=$(find test/infrastructure -name "*.bats" -exec grep -h "^@test" {} \; | wc -l)

# Conta test passati
passed_tests=$(bats test/infrastructure/**/*.bats 2>/dev/null | grep -c "‚úì")

# Calcola coverage
coverage=$((passed_tests * 100 / total_tests))

# Genera report
cat > test/reports/coverage-report.md << EOF
# Infrastructure Test Coverage Report

**Generated**: $(date)
**Total Tests**: $total_tests
**Passed Tests**: $passed_tests
**Coverage**: $coverage%

## Test Categories Coverage

$(generate_category_coverage)

## Failed Tests

$(list_failed_tests)
EOF

echo "‚úÖ Coverage report generated: test/reports/coverage-report.md"
echo "üìà Current coverage: $coverage%"
```

## üéì Best Practices e Pattern Avanzati

### ‚úÖ SOLID Principles per Test Infrastructure

#### 1. Single Responsibility Principle (SRP)

```bash
# ‚ùå ANTI-PATTERN: Test che fa troppe cose
@test "Massive test doing everything" {
    init_swarm_cluster
    deploy_monitoring_stack
    deploy_logging_stack
    test_auto_scaling
    test_zero_downtime_deployment
    verify_backup_system
    # ... troppo!
}

# ‚úÖ PATTERN CORRETTO: Test focalizzati
@test "Swarm cluster initializes correctly" {
    run init_swarm_cluster
    [ "$status" -eq 0 ]
    run is_swarm_active
    [ "$status" -eq 0 ]
}

@test "Monitoring stack deploys successfully" {
    init_swarm_cluster  # Prerequisito
    run deploy_monitoring_stack
    [ "$status" -eq 0 ]
    wait_for_service_ready "prometheus"
}
```

#### 2. Open/Closed Principle (OCP)

```bash
# Base framework estensibile
# File: test/helpers/deployment-helpers.bash

# Funzione base - chiusa per modifiche
deploy_service_base() {
    local service_name="$1"
    local image="$2"
    local replicas="$3"

    docker service create \
        --name "$service_name" \
        --replicas "$replicas" \
        "$image"
}

# Estensioni - aperte per nuove funzionalit√†
deploy_service_with_monitoring() {
    local service_name="$1"
    local image="$2"
    local replicas="$3"

    deploy_service_base "$service_name" "$image" "$replicas"

    # Estensione: aggiungi monitoring labels
    docker service update \
        --label-add "monitoring=enabled" \
        --label-add "prometheus.scrape=true" \
        "$service_name"
}

deploy_service_with_autoscaling() {
    local service_name="$1"
    local image="$2"
    local min_replicas="$3"
    local max_replicas="$4"

    deploy_service_base "$service_name" "$image" "$min_replicas"

    # Estensione: aggiungi auto-scaling
    configure_service_autoscaling "$service_name" "$min_replicas" "$max_replicas"
}
```

#### 3. Dependency Inversion Principle (DIP)

```bash
# Abstraction layer per orchestrator
# File: test/helpers/orchestrator-interface.bash

# Interfaccia astratta
create_service() {
    case "$ORCHESTRATOR" in
        "swarm")
            create_service_swarm "$@"
            ;;
        "k8s")
            create_service_k8s "$@"
            ;;
        *)
            fail "Unsupported orchestrator: $ORCHESTRATOR"
            ;;
    esac
}

# Implementazioni concrete
create_service_swarm() {
    local name="$1" image="$2" replicas="$3"
    docker service create --name "$name" --replicas "$replicas" "$image"
}

create_service_k8s() {
    local name="$1" image="$2" replicas="$3"
    kubectl create deployment "$name" --image="$image" --replicas="$replicas"
}

# Test indipendenti dall'implementazione
@test "Service creates successfully regardless of orchestrator" {
    run create_service "test-app" "nginx:alpine" 3
    [ "$status" -eq 0 ]
}
```

### üîÑ Pattern Data-Driven Testing

```bash
# File: test/fixtures/scaling-scenarios.txt
# service_name,initial_replicas,target_replicas,expected_time
web-frontend,3,10,45
api-backend,5,15,60
data-processor,2,8,30

@test "Auto-scaling scenarios from data file" {
    # Data-driven test execution
    while IFS=',' read -r service_name initial_replicas target_replicas expected_time || [[ -n "$service_name" ]]; do
        # Skip commenti e righe vuote
        [[ "$service_name" =~ ^#.*$ ]] && continue
        [[ -z "$service_name" ]] && continue

        echo "Testing scaling: $service_name ($initial_replicas -> $target_replicas)" >&3

        # Setup servizio
        run deploy_service "$service_name" "nginx:alpine" "$initial_replicas"
        [ "$status" -eq 0 ]

        # Trigger scaling
        run scale_service "$service_name" "$target_replicas"
        [ "$status" -eq 0 ]

        # Verifica timing
        run wait_for_service_scaled "$service_name" "$target_replicas" "$expected_time"
        [ "$status" -eq 0 ]

        # Cleanup
        remove_service "$service_name"

    done < "test/fixtures/scaling-scenarios.txt"
}
```

## üéØ Conclusioni e Next Steps

### üìö Riepilogo Apprendimento

**Hai imparato:**

- ‚úÖ Cos'√® BATS e perch√© √® essenziale per TDD Infrastructure
- ‚úÖ Come scrivere test da basic ad avanzati
- ‚úÖ Pattern di organizzazione test e helper functions
- ‚úÖ Integrazione con CI/CD e workflow di team
- ‚úÖ Debugging techniques e troubleshooting
- ‚úÖ Best practices e principi SOLID per test

### üöÄ Prossimi Steps

1. **Implementa il tuo primo test**: Scrivi un test BATS per una funzionalit√† infrastructure
2. **Contribuisci agli helpers**: Aggiungi una helper function utile al team
3. **Ottimizza i test esistenti**: Trova e rifattorizza duplicazioni nel codice test
4. **Misura le performance**: Aggiungi monitoring e SLA ai test critici

### üìñ Risorse Approfondimento

- **BATS Documentation**: <https://github.com/bats-core/bats-core>
- **Docker Swarm Testing**: <https://docs.docker.com/engine/swarm/>
- **Infrastructure Testing Patterns**: Martin Fowler's Testing Strategies
- **TDD Principles**: Kent Beck's "Test Driven Development"

---

**üéâ Congratulazioni!** Ora sei pronto per contribuire efficacemente ai test infrastructure del progetto e per applicare i principi TDD alla creazione di sistemi scalabili e affidabili.

---

# üîç Perch√© Non Vedo Container Dopo i Test BATS? Architettura vs Application Layer

Ottima osservazione! Questo √® un **punto fondamentale** nell'architettura Docker Swarm e nel design dei test infrastructure. Ti spiego il perch√© con un approccio step-by-step.

## üß† Il Concetto: Infrastructure vs Application Layer

```mermaid
graph TD
    A[Docker Swarm Tests] --> B[Infrastructure Layer]
    A --> C[Application Layer]

    B --> B1[Cluster Initialization]
    B --> B2[Node Management]
    B --> B3[Network Creation]
    B --> B4[Secret Management]

    C --> C1[Service Deployment]
    C --> C2[Container Instances]
    C --> C3[Load Balancing]
    C --> C4[Health Checks]

    style B fill:#e3f2fd
    style C fill:#e8f5e8
    style B1 fill:#bbdefb
    style C1 fill:#c8e6c9
```

### üéØ I Tuoi Test Stanno Validando L'INFRASTRUCTURE

I test che hai eseguito verificano che:

- ‚úÖ **Docker Swarm cluster** si possa inizializzare
- ‚úÖ **Manager nodes** funzionino correttamente
- ‚úÖ **Join tokens** vengano generati
- ‚úÖ **Leave/rejoin** operations funzionino

**NON** deployano servizi/container perch√© seguono il principio **Single Responsibility**!

## üìä Analisi: Cosa Succede Durante i Test

### Test Execution Flow

```mermaid
sequenceDiagram
    participant BATS as BATS Test Runner
    participant Docker as Docker Engine
    participant Swarm as Swarm Cluster

    BATS->>Docker: docker swarm init
    Docker->>Swarm: Create cluster
    Swarm-->>Docker: Cluster ready
    Docker-->>BATS: Success

    BATS->>BATS: Verify cluster state
    BATS->>Docker: docker node ls
    Docker-->>BATS: Manager node active

    BATS->>Docker: docker swarm join-token worker
    Docker-->>BATS: Token generated

    Note over BATS: ‚úÖ Infrastructure tests pass

    BATS->>Docker: docker swarm leave --force
    Docker->>Swarm: Destroy cluster
    Note over BATS: üßπ Cleanup completed
```

### üî¨ Verifichiamo Insieme Lo Stato

```bash
# Dopo i test, controlliamo lo stato del cluster
docker info --format '{{.Swarm.LocalNodeState}}'
# Output atteso: "inactive"

# Verifichiamo i nodi
docker node ls
# Output atteso: "Error response: This node is not a swarm manager"

# Verifichiamo i servizi
docker service ls
# Output atteso: "Error response: This node is not a swarm manager"
```

**Ecco perch√© non vedi container**: i test fanno **cleanup automatico**!

## üõ†Ô∏è Pattern Analysis: Pro e Contro del Test Design

### ‚úÖ **Pattern Scelto: Isolated Infrastructure Testing**

**Pro:**

- **Test Isolation**: Ogni test parte da stato pulito
- **Reproducibility**: Risultati consistenti
- **CI/CD Friendly**: Non lascia residui tra esecuzioni
- **Fast Execution**: Focus solo su infrastructure logic

**Contro:**

- **Limited Visibility**: Non vedi il risultato finale
- **No Integration Testing**: Manca test end-to-end
- **Learning Curve**: Pu√≤ confondere i nuovi sviluppatori

### üîÑ **Pattern Alternativo: Persistent State Testing**

**Pro:**

- **Visible Results**: Vedi container dopo i test
- **Integration Testing**: Test pi√π realistici
- **Debugging Easier**: Stato persistente per investigation

**Contro:**

- **Test Pollution**: Test precedenti influenzano successivi
- **Complex Cleanup**: Gestione stato tra test
- **Resource Consumption**: Accumula risorse nel tempo

### üéØ **Pattern Recommendato: Hybrid Approach**

Combiniamo entrambi usando **Test Categories**!

## üîß Soluzione: Estendere i Test per Visibility

### Step 1: Aggiungiamo Test con Visibilit√†

```bash
# File: test/infrastructure/cluster/test-cluster-with-services.bats
#!/usr/bin/env bats

load '../../helpers/docker-helpers'

# Setup persistent per questo specifico test file
setup_file() {
    export PERSIST_CLUSTER="true"
    export TEST_STACK_NAME="visibility-test"
}

teardown_file() {
    # Cleanup solo alla fine di tutti i test del file
    if [[ "$PERSIST_CLUSTER" == "true" ]]; then
        cleanup_docker_environment
    fi
}

@test "INTEGRATION: Cluster + Service deployment end-to-end" {
    # Given: Cluster inizializzato
    run init_swarm_cluster
    [ "$status" -eq 0 ]

    # When: Deploy servizio test
    run deploy_test_service "$TEST_STACK_NAME"
    [ "$status" -eq 0 ]

    # Then: Servizio visibile e funzionante
    run docker service ls --format "{{.Name}}"
    [[ "$output" =~ "$TEST_STACK_NAME" ]]

    run wait_for_service_ready "$TEST_STACK_NAME" 60
    [ "$status" -eq 0 ]

    # Verifica container attivi
    local container_count
    container_count=$(docker ps --filter "label=com.docker.swarm.service.name=$TEST_STACK_NAME" --format "{{.ID}}" | wc -l)
    [ "$container_count" -gt 0 ]

    echo "‚úÖ SUCCESS: $container_count containers running for service $TEST_STACK_NAME" >&3
}

@test "INTEGRATION: Service scaling works correctly" {
    # Given: Servizio gi√† deployato dal test precedente
    local initial_replicas=1
    local target_replicas=3

    # When: Scale servizio
    run docker service scale "$TEST_STACK_NAME=$target_replicas"
    [ "$status" -eq 0 ]

    # Then: Replicas corrette
    run wait_for_service_scaled "$TEST_STACK_NAME" "$target_replicas" 60
    [ "$status" -eq 0 ]

    local container_count
    container_count=$(docker ps --filter "label=com.docker.swarm.service.name=$TEST_STACK_NAME" --format "{{.ID}}" | wc -l)
    [ "$container_count" -eq "$target_replicas" ]

    echo "‚úÖ SUCCESS: Scaled to $container_count containers" >&3
}
```

### Step 2: Helper Functions per Service Management

```bash
# File: test/helpers/service-helpers.bash

# Deploy servizio test semplice
deploy_test_service() {
    local service_name="$1"
    local image="${2:-nginx:alpine}"
    local replicas="${3:-1}"

    docker service create \
        --name "$service_name" \
        --replicas "$replicas" \
        --publish "8080:80" \
        --label "test=true" \
        "$image" >/dev/null 2>&1
}

# Attendi scaling completo
wait_for_service_scaled() {
    local service_name="$1"
    local expected_replicas="$2"
    local timeout="${3:-120}"

    local start_time=$(date +%s)

    while true; do
        local current_replicas
        current_replicas=$(docker service ls --filter "name=$service_name" --format "{{.Replicas}}" | cut -d'/' -f1)

        if [[ "$current_replicas" -eq "$expected_replicas" ]]; then
            return 0
        fi

        local elapsed=$(($(date +%s) - start_time))
        if [[ "$elapsed" -gt "$timeout" ]]; then
            return 1
        fi

        sleep 2
    done
}

# Verifica servizio healthy
verify_service_healthy() {
    local service_name="$1"
    local endpoint="${2:-http://localhost:8080}"

    # Test connettivit√†
    curl -s -f "$endpoint" >/dev/null
}
```

### Step 3: Makefile per Diverse Modalit√† Test

```makefile
# File: Makefile

.PHONY: test-infra-quick test-infra-with-visibility test-infra-cleanup

# Test rapidi (isolation pattern)
test-infra-quick:
 @echo "üîß Running isolated infrastructure tests..."
 @bats test/infrastructure/cluster/test-cluster-init.bats

# Test con visibilit√† (persistent pattern)
test-infra-with-visibility:
 @echo "üëÅÔ∏è Running infrastructure tests with persistent state..."
 @bats test/infrastructure/cluster/test-cluster-with-services.bats
 @echo ""
 @echo "üîç Current cluster state:"
 @docker node ls 2>/dev/null || echo "No cluster active"
 @echo ""
 @echo "üìã Active services:"
 @docker service ls 2>/dev/null || echo "No services active"
 @echo ""
 @echo "üê≥ Running containers:"
 @docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

# Cleanup manuale
test-infra-cleanup:
 @echo "üßπ Cleaning up test environment..."
 @./scripts/cleanup-test-environment.sh
 @echo "‚úÖ Environment cleaned"

# Help
help-testing:
 @echo "üß™ Available testing modes:"
 @echo "  test-infra-quick           - Fast isolated tests (no visible containers)"
 @echo "  test-infra-with-visibility - Integration tests (containers remain visible)"
 @echo "  test-infra-cleanup         - Manual cleanup of test environment"
```

## üöÄ Prova la Nuova Modalit√†

### Esegui Test con Visibilit√†

```bash
# 1. Esegui test che lasciano stato visibile
make test-infra-with-visibility

# 2. Verifica lo stato
docker node ls
docker service ls
docker ps

# 3. Testa l'applicazione
curl http://localhost:8080

# 4. Cleanup quando finito
make test-infra-cleanup
```

### Output Atteso

```bash
üê≥ Running containers:
NAMES                               STATUS              PORTS
visibility-test.1.abc123def456      Up 2 minutes        *:8080->80/tcp

üìã Active services:
ID             NAME             MODE         REPLICAS   IMAGE
xyz789         visibility-test  replicated   3/3        nginx:alpine
```

## üéØ TDD Extension: Test "Given-When-Then"

### Nuovi Test Scenarios

```gherkin
Scenario: Infrastructure foundation supporta application deployment
  Given Docker Swarm cluster √® inizializzato
  When deploy servizio nginx con 3 replicas
  Then dovrei vedere 3 container attivi
  And servizio dovrebbe rispondere su porta 8080
  And load balancing dovrebbe distribuire richieste

Scenario: Cluster supporta zero-downtime deployment
  Given servizio nginx v1.0 running con 3 replicas
  When aggiorno a nginx v2.0 usando rolling update
  Then servizio rimane sempre raggiungibile
  And alla fine tutte le replicas sono v2.0
  And zero dropped connections durante update

Scenario: Auto-recovery funziona correttamente
  Given servizio running con 3 replicas
  When simulo failure di 1 replica
  Then Swarm dovrebbe restart replica automaticamente
  And target replicas dovrebbe essere mantenuto
  And servizio performance non dovrebbe degradare
```

## üìä Principi SOLID Applicati

### **Single Responsibility Principle**

- Test isolation: ogni test ha responsabilit√† specifica
- Service helpers: funzioni focalizzate su singola operazione

### **Open/Closed Principle**

- Framework test estensibile per nuovi scenari
- Helper functions modulari e componibili

### **Dependency Inversion**

- Test dipendono da interfacce (helper functions)
- Non dipendono da implementazioni specifiche Docker

## üéì Key Takeaways

### üß† **Concetti Appresi**

1. **Infrastructure vs Application Testing**: Differenza fondamentale tra test di setup e test di funzionalit√†
2. **Test Isolation**: Vantaggi e svantaggi del cleanup automatico
3. **Hybrid Testing Strategy**: Combinare approcci per massima effectiveness

### üõ†Ô∏è **Tool e Pattern**

- **BATS Test Categories**: Organizzazione test per scopo
- **Makefile Integration**: Command interface per diverse modalit√†
- **Helper Function Design**: Modularit√† e riutilizzabilit√†

### üöÄ **Prossimi Steps**

1. **Estendi i test**: Aggiungi scenari application-level
2. **Monitoring Integration**: Test per Prometheus/Grafana deployment
3. **CI/CD Pipeline**: Integrazione con GitHub Actions

---

**üí° Risposta alla tua domanda**: Non vedevi container perch√© i test BATS erano progettati per validare solo l'infrastructure layer con cleanup automatico. Ora hai gli strumenti per vedere e testare anche l'application layer!

Vuoi che procediamo implementando il test per il deployment del monitoring stack (Prometheus/Grafana)?

---

_Documento aggiornato: 15 Settembre 2025_  
_Versione: 1.0_  
_Autore: Tech Lead Team_
